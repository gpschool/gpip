---
layout: default
title: Further Reading
hide: True
---

<h3>Background Reading</h3>

<a name="Rasmussen:book06"></a><span class=author>C. E. Rasmussen and C. K. I. Williams. </span> (2006) <span class=papertitle>"Gaussian processes for machine learning"</span>, MIT Press, Cambridge, MA.  

<h4>Synopsis</h4>

<p class=abstract>Gaussian Processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decase, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targetd at researchers and students in machine learning and applied statistics.<br><br> The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines, and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.
<hr />

<a name="Quinonero:unifying05"></a><span class=author>J.Qui&#241onero Candela and C. E. Rasmussen. </span> (2005) <span class=papertitle>"A unifying view of sparse approximate Gaussian process regression"</span> in <span class=journal>Journal of Machine Learning Research</span>  6, pp 1939--1959  [<a href=http://jmlr.csail.mit.edu/papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf>PDF</a>][<a href="http://scholar.google.com/scholar?hl-en&lr=&q=A+Unifying+View+of+Sparse+Approximate+Gaussian+Process+Regression+&btnG=Search">Google Scholar Search</a>]

<h4>Abstract</h4>

<p class=abstract>We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the <em>effective prior</em> which the methods are using. This allows new insights to be gained, and highlights the relashionship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.
<hr />

<a name="Engel:octopus05"></a><span class=author>Y. Engel, P. Szabo and D. Volkinshtein. </span> (2006) <span class=papertitle>"Learning to control an Octopus arm with Gaussian process temporal difference methods"</span> in Y. Weiss, B. Sch&#246lkopf and J. C. Platt (eds) <span class=journal>Advances in Neural Information Processing Systems</span>, MIT Press, Cambridge, MA. [<a href=http://www.cs.ualberta.ca/~yaki/papers/gprl_icml05_camera.ps>Postscript</a>][<a href=http://www.cs.ualberta.ca/~yaki/papers/gprl_icml05_camera.pdf>PDF</a>][<a href="http://scholar.google.com/scholar?hl-en&lr=&q=Learning+to+Control+an+Octopus+Arm+with+Gaussian+Process+Temporal+Difference+Methods+&btnG=Search">Google Scholar Search</a>]

<h4>Abstract</h4>

<p class=abstract>The Octopus arm is a highly versatile and complex limb. How the Octopus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical principles may render present day robotic arms obsolete. In this paper, we tackle this control problem using an online reinforcement learning algorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning. Our substitute for the real arm is a computer simulation of a 2-dimensional model of an Octopus arm. Even with the simplifications inherent to this model, the state space we face is a high-dimensional one. We apply a GPTD-based algorithm to this domain, and demonstrate its operation on several learning tasks of varying degrees of difficulty.
<hr />



<a name="Urtasun:priors05"></a><span class=author>R. Urtasun, D. J. Fleet, A. Hertzmann and P. Fua. </span> (2005) <span class=papertitle>"Priors for people tracking from small training sets"</span> in <span class=journal>IEEE International Conference on Computer Vision (ICCV)</span>, IEEE Computer Society Press, Bejing, China. [<a href=http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnumber=32910&arnumber=1541284&count=120&index=52>IEEE Xplore</a>][<a href=http://cvlab.epfl.ch/~rurtasun/publications/urtasun_etal_iccv05.pdf>PDF</a>][<a href="http://scholar.google.com/scholar?hl-en&lr=&q=Priors+for+People+Tracking+from+Small+Training+Sets+&btnG=Search">Google Scholar Search</a>]

<h4>Abstract</h4>

<p class=abstract>We advocate the use of Scaled Gaussian Process Latent Variable Models (SGPLVM) to learn prior models of 3D human pose for 3D people tracking. The SGPLVM simultaneously optimizes a low-dimensional embedding of the high dimensional pose data and a density function that both gives higher probability to points close to training data and provides a nonlinear probabilistic mapping from the low dimensional latent space to the full-dimensional pose space. The SGPLVM is a natural choice when only small amounts of training data are available. We demonstrate our approach with two distinct motions, golfing and walking. We show that the SGPLVM sufficiently constrains the problem such that tracking can be accomplished with straightforward deterministic optimization.
<hr />

<a name="Grochow:styleik04"></a><span class=author>K. Grochow, S. L. Martin, A. Hertzmann and Z. Popovic. </span> (2004) <span class=papertitle>"Style-based inverse kinematics"</span> in <span class=journal>ACM Transactions on Graphics (SIGGRAPH 2004)</span>, [<a href=http://grail.cs.washington.edu/projects/styleik/styleik.pdf>PDF</a>][<a href=http://grail.cs.washington.edu/projects/styleik/>Web page</a>][<a href="http://scholar.google.com/scholar?hl-en&lr=&q=Style-Based+Inverse+Kinematics+&btnG=Search">Google Scholar Search</a>]

<h4>Abstract</h4>

<p class=abstract>This paper presents an inverse kinematics system based on a learned model of human poses. Given a set of constraints, our system can produce the most likely pose satisfying those constraints, in real time. Training the model on different input data leads to different styles of IK. The model is represented as a probability distribution over the space of all possible poses. This means that our IK system can generate any pose, but prefers poses that are most similar to the space of poses in the training data. We represent the probability with a novel model called a Scaled Gaussian Process Latent Variable Model. The parameters of the model are all learned automatically; no manual tuning is required for the learning component of the system. We additionally describe a novel procedure for interpolating between styles.<br><br> Our style-based IK can replace conventional IK, wherever it is used in computer animation and computer vision. We demonstrate our system in the context of a number of applications: interactive character posing, trajectory keyframing, real-time motion capture with missing markers, and posing from a 2D image.
<hr />

<a name="Williams:prediction98"></a><span class=author>C. K. I. Williams</span> (1998) <span class=papertitle>"Prediction with Gaussian processes: from linear regression to linear prediction and beyond"</span> in M. I. Jordan (ed.) <span class=journal>Learning in Graphical Models</span>, Kluwer, Dordrecht, The Netherlands.  [<a href=http://www.dai.ed.ac.uk/homes/ckiw/postscript/NCRG_97_012.ps.gz>Gzipped Postscript</a>][<a href="http://scholar.google.com/scholar?hl-en&lr=&q=Prediction+with+Gaussian+Processes:+From+Linear+Regression+to+Linear+Prediction+and+Beyond+&btnG=Search">Google Scholar Search</a>]

<h4>Abstract</h4>

<p class=abstract>The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads into a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions fro neural network models and the use of Gaussian processes in classification problems.
<hr />

<a name="MacKay:gpintroduction98"></a><span class=author>D. J. C.MacKay</span> (1998) <span class=papertitle>"Introduction to Gaussian Processes"</span> in C. M. Bishop (ed.) <span class=journal>Neural Networks and Machine Learning</span>, Springer-Verlag, Berlin.  [<a href=http://www.cs.toronto.edu/~mackay/gpB.ps.gz>Gzipped Postscript</a>][<a href="http://scholar.google.com/scholar?hl-en&lr=&q=Introduction+to+Gaussian+Processes+&btnG=Search">Google Scholar Search</a>]

<h4>Abstract</h4>

<p class=abstract>Feedforward neural networks such as multilayer perceptrons are popular tools for nonlinear regression and classification problems. From a Bayesian perspective, a choice of a neural network model can be viewed as defining a prior probability distribution over non-linear functions, and the neural network's learning process can be interpreted in terms of the posterior probability distribution over the unknown function. (Some learning algorithms search for the function with maximum posterior probability and other Monte Carlo methods draw samples from this posterior probability). In the limit of large but otherwise standard networks, <a href=http://ext.dcs.shef.ac.uk/~u0015/cgi-bin/bibpage.cgi?keyName=Neal:book96&printAbstract=1>[Neal:book96]</a> has shown that the prior distribution over non-linear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes. The hyperparameters of the neural network model determine the characteristic lengthscales of the Gaussian process. Neal's observation motivates the idea of discarding parameterized networks and working directly with Gaussian processes. Computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the Gaussian process. In this chapter I will review work on this idea by <a href=http://ext.dcs.shef.ac.uk/~u0015/cgi-bin/bibpage.cgi?keyName=Williams:Gaussian96&printAbstract=1>[Williams:Gaussian96]</a>, <a href=http://ext.dcs.shef.ac.uk/~u0015/cgi-bin/bibpage.cgi?keyName=Neal:montecarlogp97&printAbstract=1>[Neal:montecarlogp97]</a>, <a href=http://ext.dcs.shef.ac.uk/~u0015/cgi-bin/bibpage.cgi?keyName=Barber:Gaussian97&printAbstract=1>[Barber:Gaussian97]</a> and <a href=http://ext.dcs.shef.ac.uk/~u0015/cgi-bin/bibpage.cgi?keyName=Gibbs:variational00&printAbstract=1>[Gibbs:variational00]</a>, and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded.
<hr />

